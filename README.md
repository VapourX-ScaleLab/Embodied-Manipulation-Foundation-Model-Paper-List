# Embodied-Manipulation-Foundation-Model-Paper-List

A curated collection of influential research papers in robotics, computer vision, and machine learning.

author: [Congsheng (ACondaway) Xu](https://github.com/ACondaway), Organization: [VapourX](https://github.com/VapourX-ScaleLab)
---

## OpenVLA Series

- **[OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/pdf/2406.09246)** - Embodied manipulation VLA foundation model (2024-06)
- **[Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success](https://arxiv.org/pdf/2502.19645)** - Embodied manipulation VLA foundation model (2025-02-01)

## RDT Series

- **[RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation](https://arxiv.org/pdf/2410.07864)** - Bimanual manipulation foundation model (2024-10)
- **[H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation](https://www.arxiv.org/pdf/2507.23523)** - Data-efficient bimanual manipulation foundation model (2024-10)

## TikTok GR Series

- **[UNLEASHING LARGE-SCALE VIDEO GENERATIVE PRE-TRAINING FOR VISUAL ROBOT MANIPULATION](https://arxiv.org/pdf/2312.13139)** - Large-scale video pre-training model proposed by ByteDance (2023-12)
- **[GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation](https://arxiv.org/pdf/2410.06158)** - Large-scale video pre-training model proposed by ByteDance (2024-10)
- **[GR-3 Technical Report](https://arxiv.org/html/2507.15493v1)** - Large-scale video pre-training model proposed by ByteDance (2025-07-01)

## Google-Research RT Series

- **[RT-1: Robotics Transformer for Real-World Control at Scale](https://arxiv.org/pdf/2212.06817)** - Key work in RT series VLA (2022-12)
- **[RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control](https://arxiv.org/pdf/2307.15818)** - Key work in RT series VLA (2023-07)

## PaLM-E Series

- **[PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/pdf/2303.03378)** - Key work in PaLM-E series (2023-03)

## Meta-AI Series

- **[R3M: A Universal Visual Representation for Robot Manipulation](https://arxiv.org/pdf/2203.12601)** - Key work in Meta-AI series (2022-03)

## π Series

- **[π0: A Vision-Language-Action Flow Model for General Robot Control](https://arxiv.org/pdf/2410.24164v1)** - Key work in PI series VLA (2024-10)
- **[π0.5: a Vision-Language-Action Model with Open-World Generalization](https://arxiv.org/pdf/2410.24164v1)** - Key work in PI series VLA (2024-10)

## Being-Beyond Series

- **[Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/pdf/2507.15597)** - Building embodied manipulation foundation model using existing large-scale data (2025-07)
- **[Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills](https://arxiv.org/pdf/2503.12533?)** - Building embodied manipulation foundation model using existing large-scale data (2025-03)

## Agibot Series

- **[AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems](https://arxiv.org/pdf/2503.06669)** - Key work in Agibot series (2025-03)
- **[Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation](https://arxiv.org/pdf/2508.05635v1)** - Key work in Agibot series (2025-08)

---
## Embodied-R1 Series

- **[Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation](https://arxiv.org/pdf/2508.13998)** - Use R1-based method (2025-08)

## Galaxea Series

- **[Galaxea Open-World Dataset & G0 Dual-System VLA Model](https://github.com/OpenGalaxea/G0/blob/main/Galaxea_G0_report.pdf)** - Key works for Galaxea (2025-08)

## NVIDIA GR00T

- **[GR00T N1: An Open Foundation Model for Generalist Humanoid Robots](https://arxiv.org/pdf/2503.14734)** - Key works for NVIDIA Robotics (2025-03)

## Octo Series

- **[Octo: An Open-Source Generalist Robot Policy](https://arxiv.org/abs/2405.12213)** - (2024-05)
*Last updated: Aug.10 2025*
